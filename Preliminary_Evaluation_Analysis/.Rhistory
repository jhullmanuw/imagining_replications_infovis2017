guessed_x = as.numeric(strsplit(as.character(row["binx"]), "_")[[1]])
kld_continuous(transfer_true_sampling_ref_dist, guessed_x, guessed_y)
})
# method 1 for true sampling dist
df2cp$observed_sampling_kl = apply(df2cp, 1, function(row) {
guessed_y = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
guessed_x = as.numeric(strsplit(as.character(row["binx"]), "_")[[1]])
kld_continuous(transfer_observed_sampling_ref_dist, guessed_x, guessed_y)
})
df2cp %>%
ggplot(aes(x = log(kl_disc), y = log(kl_disc_zeros))) +
geom_point(position=position_jitter(width=.01, height=.01), alpha=.1, pch=19, size=2) +
geom_abline(intercept = 0, slope = 1) +
facet_wrap(~ condition)
df2cp %>%
ggplot(aes(x = log(kl_disc), y = log(kl_cont_zeros))) +
geom_point(position=position_jitter(width=.01, height=.01), alpha=.1, pch=19, size=2) +
geom_abline(intercept = 0, slope = 1) +
facet_wrap(~ condition)
adfp <- rbind(df2dp, df2cp)
adfp = merge(adfp, worker_prescore, by = intersect(names(adfp), names(worker_prescore)))
adfp %<>% mutate(
discrete = ifelse(condition == "d_p" | condition == "d_np" | condition == "r_np", 1, 0),
predict = ifelse(condition == "d_p" | condition == "c_p", 1, 0),
rules = ifelse(condition == "rd_np" | condition == "rc_np", 1, 0)
)
adfp %>%
mutate(condition = reorder(condition, log(kl_disc))) %>%
ggplot(aes(x = condition, y = log(kl_disc))) +
geom_violin(fill = "gray75") +
geom_boxplot(width = 0.1, color = "red", outlier.color = NA) +
geom_jitter(pch=20, width = .2)
ddply(adfp, ~condition, summarise,
mean=mean(merrs, na.rm=TRUE), sd=sd(merrs, na.rm=TRUE),
median = median(merrs, na.rm=TRUE),
mad = mad(merrs, na.rm=TRUE))
ddply(adfp, ~condition, summarise,
mean=mean(serrs, na.rm=TRUE), sd=sd(serrs, na.rm=TRUE),
median = median(serrs, na.rm=TRUE),
mad = mad(serrs, na.rm=TRUE))
#plots
adfp %>%
ggplot(aes(x = means - 0.7)) + stat_density() +
facet_wrap(~condition) +
geom_vline(xintercept = 0, color="blue") + theme(axis.text.y=element_blank(),axis.ticks.y=element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + labs(x="Predicted Mean - Reference Mean",  y="") + geom_vline(xintercept = 0.05, color="black")
adfp %>%
ggplot(aes(x = sds - sqrt(2)*(1.1/sqrt(8)))) + stat_density() +
facet_wrap(~condition) +
geom_vline(xintercept = 0, color="blue") + theme(axis.text.y=element_blank(),axis.ticks.y=element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + labs(x="Predicted SD - Reference SD",  y="") + geom_vline(xintercept = -0.20, color="black") + geom_vline(xintercept=0.55, color="orange")
#center prescore
adfp$prescore_c <- adfp$prescore - mean(adfp$prescore)
adfp_gamlss = adfp %>%
dplyr::select(discrete, predict, rules, prescore_c, kl_disc, kl_disc_zeros, kl_cont_zeros, workerId) #gamlss needs all columns without NAs
mg = gamlss(log(kl_disc) ~ discrete*predict + rules+prescore_c,
sigma.formula = ~ discrete*predict + rules+prescore_c,
data=adfp_gamlss)
summary(mg)
m_transfer_disc = map2stan(kld_model_spec, data = adfp_gamlss %>% rename(kl = kl_disc))
m_transfer_disc_zeros = map2stan(kld_model_spec, data = adfp_gamlss %>% rename(kl = kl_disc_zeros))
m_transfer_cont_zeros = map2stan(kld_model_spec, data = adfp_gamlss %>% rename(kl = kl_cont_zeros))
plot_kld_model_mu_coefs(m_transfer_disc) + xlim(-1.5, 1.5) + ggtitle("Method 1")
plot_kld_model_mu_coefs(m_transfer_disc_zeros) + xlim(-1.5, 1.5) + ggtitle("Method 2")
plot_kld_model_mu_coefs(m_transfer_cont_zeros) + xlim(-1.5, 1.5) + ggtitle("Method 3")
plot_kld_model_sigma_coefs(m_transfer_disc) + xlim(-1.5, 1.5) + ggtitle("Method 1")
plot_kld_model_sigma_coefs(m_transfer_disc_zeros) + xlim(-1.5, 1.5) + ggtitle("Method 2")
plot_kld_model_sigma_coefs(m_transfer_cont_zeros) + xlim(-1.5, 1.5) + ggtitle("Method 3")
plot_kld_model_expected_mu(m_transfer_disc) + xlim(-2, 1) + ggtitle("Method 1")
plot_kld_model_expected_mu(m_transfer_disc_zeros) + xlim(-1.5, 1) + ggtitle("Method 2")
plot_kld_model_expected_mu(m_transfer_cont_zeros) + xlim(-1.5, 1) + ggtitle("Method 3")
plot_kld_model_expected_sigma(m_transfer_disc) + xlim(-1, 1.5) + ggtitle("Method 1")
plot_kld_model_expected_sigma(m_transfer_disc_zeros) + xlim(-1, 1.5) + ggtitle("Method 2")
plot_kld_model_expected_sigma(m_transfer_cont_zeros) + xlim(-1, 1.5) + ggtitle("Method 3")
#for comparing population sampling dist and observed sampling dist to see if some participants are confused by the difference
adfp_gamlssp = adfp %>%
dplyr::select(discrete, predict, rules, prescore, true_sampling_kl) #gamlss needs all columns without NAs
mg = gamlss(log(true_sampling_kl) ~ discrete*predict + rules+prescore,
sigma.formula = ~ discrete*predict + rules+prescore,
data=adfp_gamlssp)
summary(mg)
#Overall
ddply(adsL,~tasktype,summarise,mean=mean(erra, na.rm=TRUE),sd=sd(erra, na.rm=TRUE))
ggplot(adsL, aes(x=reorder(tasktype, erra, FUN=median), erra)) + geom_boxplot() + geom_jitter() + facet_wrap( ~ q_num) #useless chart
#Linear mixed effects model -- first in lme4
full.model <- lmer(erra  ~ discrete + predict + prescore + discrete*predict + rules + (1+prescore| workerId) + (1+prescore| q_num), data=adsL)
summary(full.model)
coefs <- data.frame(coef(summary(full.model)))
# use normal distribution to approximate p-value
coefs$p.z <- 2 * (1 - pnorm(abs(coefs$t.value)))
coefs
myvars <- c("q_num", "workerId", "erra", "discrete", "predict", "rules", "prescore")
adsL_stan <- adsL[myvars]
# construct subject index --- glmer2stan forces you to manage your own cluster indices
adsL_stan$subject_index <- as.integer(as.factor(adsL_stan$workerId))
adsL_stan$subject_index = adsL_stan$subject_index - 1
# fit with glmer2stan
m1_g2s <- lmer2stan( erra ~ discrete + predict + discrete*predict + rules + prescore + (1| subject_index) + (1 | q_num) , data=adsL_stan )
stanmer(m1_g2s)
posterior <- extract(m1_g2s)
plot(m1_g2s)
stan_plot(m1_g2s)
plot(m1_g2s)
ggplot() + geom_violin(data=violin, mapping=aes(x=condition, estimate),fill="black") + coord_flip() + geom_hline(aes(yintercept=0, colour="red"))+ guides(color=FALSE) + labs(y="Estimate", x="")
coef_continuous = extract.samples(m1_g2s,n=10000) %$% {Intercept}
coef_discrete = extract.samples(m1_g2s,n=10000) %$% {beta_discrete}
coef_predict = extract.samples(m1_g2s,n=10000) %$% {beta_predict}
coef_discrete_predict = extract.samples(m1_g2s) %$% {beta_discrete_X_predict}
coef_rules = extract.samples(m1_g2s,n=10000) %$% {beta_rules}
coef_prescore = extract.samples(m1_g2s,n=10000) %$% {beta_prescore}
violin <- data.frame(cbind(coef_continuous,coef_predict,coef_discrete,coef_discrete_predict,coef_rules,coef_prescore))
violin <- reshape(varying = c("coef_continuous","coef_predict","coef_discrete","coef_discrete_predict","coef_rules","coef_prescore"), v.names = "estimate",  timevar="condition", times=c("coef_continuous","coef_predict","coef_discrete","coef_discrete_predict","coef_rules","coef_prescore"), direction = "long", data=violin)
violin$condition <- ifelse(violin$condition=="coef_continuous", "intercept", ifelse(violin$condition=="coef_rules", "rules", ifelse(violin$condition=="coef_predict", "predict", ifelse(violin$condition=="coef_discrete_predict", "discrete*predict", ifelse(violin$condition=="coef_discrete", "discrete", "prescore")))))
ggplot() + geom_violin(data=violin, mapping=aes(x=condition, estimate),fill="black") + coord_flip() + geom_hline(aes(yintercept=0, colour="red"))+ guides(color=FALSE) + labs(y="Estimate", x="")
beta_continuous_posterior = extract.samples(m1_g2s,n=10000) %$% {Intercept}
beta_discrete_posterior = extract.samples(m1_g2s,n=10000) %$% {Intercept + beta_discrete}
beta_predict_posterior = extract.samples(m1_g2s,n=10000) %$% {Intercept + beta_predict}
beta_discrete_predict_posterior = extract.samples(m1_g2s) %$% {Intercept + beta_discrete + beta_predict + beta_discrete_X_predict}
beta_rules_posterior = extract.samples(m1_g2s,n=10000) %$% {Intercept + beta_rules}
beta_discrete_rules_posterior = extract.samples(m1_g2s,n=10000) %$% {Intercept + beta_rules + beta_discrete}
violin <- data.frame(cbind(beta_continuous_posterior,beta_predict_posterior,beta_discrete_posterior,beta_discrete_predict_posterior,beta_rules_posterior,beta_discrete_rules_posterior))
violin <- reshape(varying = c("beta_continuous_posterior","beta_predict_posterior","beta_discrete_posterior","beta_discrete_predict_posterior","beta_rules_posterior","beta_discrete_rules_posterior"), v.names = "estimate",  timevar="condition", times=c("beta_continuous_posterior","beta_predict_posterior","beta_discrete_posterior","beta_discrete_predict_posterior","beta_rules_posterior","beta_discrete_rules_posterior"), direction = "long", data=violin)
violin$condition <- ifelse(violin$condition=="beta_continuous_posterior", "continuous none", ifelse(violin$condition=="beta_rules_posterior", "continuous rules", ifelse(violin$condition=="beta_predict_posterior", "continuous predict", ifelse(violin$condition=="beta_discrete_predict_posterior", "discrete predict", ifelse(violin$condition=="beta_discrete_rules_posterior", "discrete rules", "discrete none")))))
ggplot() + geom_violin(data=violin, mapping=aes(x=condition, estimate),fill="black") + coord_flip() + geom_hline(aes(yintercept=0, colour="red"))+ guides(color=FALSE) + labs(y="Estimate", x="")
plot(m1_g2s)
packageVersion("ggplot2")
cat("\014")
library(dplyr)
import::from(plyr, ddply, rename)
library(magrittr)   #pipe syntax (%>%, %<>%, etc)
library(ggplot2)
library(lme4)
import::from(gamlss.dist, dTF, qTF, pTF, rTF)   #the TF functions are a scaled and shifted t distribution
import::from(LaplacesDemon, KLD)
import::from(MASS, parcoord)
library(rstan)
# install via devtools::install_github("rmcelreath/glmer2stan")
library(glmer2stan)
# install via devtools::install_github("rmcelreath/rethinking")
library(rethinking)
theme_set(theme_bw())
options(mc.cores = parallel::detectCores())
mu = 10
sd = 2
min_x = 0
max_x = 20
interval_size = 2
#derive discrete reference distribution
make_discrete_reference_dist = function(dist_fun, min_x, max_x, interval_size) {
data_frame(
x_min = seq(min_x, max_x - interval_size, by = interval_size),
x_max = seq(min_x + interval_size, max_x, by = interval_size),
x = (x_min + x_max)/2,   #midpoint of the interval
#this difference sometimes comes out as 0 due to rounding errors in bins with very low probability
#adjust it to always be at least epsilon (smallest value > 0 on this machine
#such that x + eps != x is always true).
p = pmax(.Machine$double.eps, dist_fun(x_max) - dist_fun(x_min))
)
}
#derive discrete analog
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
#remember to change to interval_size=1 for 100 balls
#check what it looks like:
correct_dist %>%
ggplot(aes(x = x, y = p)) +
stat_function(fun = function(x) dnorm(x, mu, sd) * interval_size) +
geom_segment(aes(xend = x, y = 0, yend = p), size = 2)
df <- read.csv("anonymized_survey_response_dists_with_order_forR_infovis.txt", sep="\t", row.names=NULL)
df2 <- read.csv("anonymized_survey_turk_response_dists_with_order_forR_infovis.txt", sep="\t", row.names=NULL)
colnames(df)
df20 <- subset(df, df$nballs=="20")
df50 = subset(df, df$nballs=="50")
df100 = subset(df, df$nballs=="100")
df220 <- subset(df2, df2$nballs=="20")
df250 = subset(df2, df2$nballs=="50")
df2100 = subset(df2, df2$nballs=="100")
#for 20 and 50 circles
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
#function to calculate
kld_discrete = function(reference_dist, guesses) {
#responses with 20 balls appear to be recorded as being out of 100, so
#instead of assuming some number here we'll just make this out of the
#total of the supplied guesses
guesses = guesses / sum(guesses)
smoothing_denominator = 1000
n_atoms = nrow(correct_dist)
#make sure the number of supplied guesses is the same as the atoms in the reference dist
stopifnot(n_atoms == length(guesses))
#calculate smoothed guesses (guaranteed to have no 0s and sum to 1)
estimated_p = (guesses * smoothing_denominator + 1) / (smoothing_denominator + n_atoms)
#assert we have normalized everything correctly (within a reasonable tolerance)
# stopifnot(isTRUE(all.equal(sum(reference_dist$p), 1, tolerance = 0.00001)))
#  stopifnot(isTRUE(all.equal(sum(estimated_p), 1, tolerance = 0.00001)))
#since we have normalized everything correctly, we can use the KLD formula directly
#instead of calling KLD (which renormalizes again)
sum(reference_dist$p * (log(reference_dist$p) - log(estimated_p)))
}
df20$kl = apply(df20, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
#turk data
df220$kl = apply(df220, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df50$kl = apply(df50, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
#turk data
df250$kl = apply(df250, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
interval_size = 1
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
df100$kl = apply(df100, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df2100$kl = apply(df2100, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
cat("\014")
getwd()
setwd("../Preliminary_Evaluation_Analysis/")
theme_set(theme_bw())
options(mc.cores = parallel::detectCores())
mu = 10
sd = 2
min_x = 0
max_x = 20
interval_size = 2
#derive discrete reference distribution
make_discrete_reference_dist = function(dist_fun, min_x, max_x, interval_size) {
data_frame(
x_min = seq(min_x, max_x - interval_size, by = interval_size),
x_max = seq(min_x + interval_size, max_x, by = interval_size),
x = (x_min + x_max)/2,   #midpoint of the interval
#this difference sometimes comes out as 0 due to rounding errors in bins with very low probability
#adjust it to always be at least epsilon (smallest value > 0 on this machine
#such that x + eps != x is always true).
p = pmax(.Machine$double.eps, dist_fun(x_max) - dist_fun(x_min))
)
}
#derive discrete analog
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
#remember to change to interval_size=1 for 100 balls
#check what it looks like:
correct_dist %>%
ggplot(aes(x = x, y = p)) +
stat_function(fun = function(x) dnorm(x, mu, sd) * interval_size) +
geom_segment(aes(xend = x, y = 0, yend = p), size = 2)
df <- read.csv("anonymized_survey_response_dists_with_order_forR_infovis.txt", sep="\t", row.names=NULL)
df2 <- read.csv("anonymized_survey_turk_response_dists_with_order_forR_infovis.txt", sep="\t", row.names=NULL)
colnames(df)
df20 <- subset(df, df$nballs=="20")
df50 = subset(df, df$nballs=="50")
df100 = subset(df, df$nballs=="100")
df220 <- subset(df2, df2$nballs=="20")
df250 = subset(df2, df2$nballs=="50")
df2100 = subset(df2, df2$nballs=="100")
#for 20 and 50 circles
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
#function to calculate
kld_discrete = function(reference_dist, guesses) {
#responses with 20 balls appear to be recorded as being out of 100, so
#instead of assuming some number here we'll just make this out of the
#total of the supplied guesses
guesses = guesses / sum(guesses)
smoothing_denominator = 1000
n_atoms = nrow(correct_dist)
#make sure the number of supplied guesses is the same as the atoms in the reference dist
stopifnot(n_atoms == length(guesses))
#calculate smoothed guesses (guaranteed to have no 0s and sum to 1)
estimated_p = (guesses * smoothing_denominator + 1) / (smoothing_denominator + n_atoms)
#assert we have normalized everything correctly (within a reasonable tolerance)
# stopifnot(isTRUE(all.equal(sum(reference_dist$p), 1, tolerance = 0.00001)))
#  stopifnot(isTRUE(all.equal(sum(estimated_p), 1, tolerance = 0.00001)))
#since we have normalized everything correctly, we can use the KLD formula directly
#instead of calling KLD (which renormalizes again)
sum(reference_dist$p * (log(reference_dist$p) - log(estimated_p)))
}
df20$kl = apply(df20, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
#turk data
df220$kl = apply(df220, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df50$kl = apply(df50, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
#turk data
df250$kl = apply(df250, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
interval_size = 1
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
df100$kl = apply(df100, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df2100$kl = apply(df2100, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
min_x = 0
max_x = 20
mu = 10
sd = 2
interval_size = 2
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
y_to_density = function(x, estimated_y) {
user_density = approxfun(x, estimated_y)
area_under_curve = integrate(user_density, lower = min(x), upper = max(x))$value
#finally, a similar smoothing as applied in the discrete case to ensure there are no 0s
#see kl-divergence-for-discrete-dist.Rmd
#might want to revisit this smoothing in the other files and make them all exactly the same?
#(i.e. here we assume fractions out of 1000, while in the other file we used 100 and 20 depending
#on number of balls --- but this could make the KLD higher for those conditions just because of
#differences in smoothing parameters)
(estimated_y / area_under_curve * 1000 + 1) / (1000 + length(x))
}
dfc=subset(df, df$condition == "drawcontline" | df$condition=="drawcontfill")
dfc2=subset(df2, df2$condition == "drawcontline" | df2$condition=="drawcontfill")
klsc <- vector()
for(i in 1:length(dfc$binprob)){
estimated_y = strsplit(as.character(dfc$binprob[i]), ",")
estimated_y = as.numeric(estimated_y[[1]])
#interval_size = 1
guesses = correct_dist %>%
mutate(
estimated_p = y_to_density(c(2,4,6,8,10,12,14,16,18,20), estimated_y)
)
klsc = append(klsc, guesses %$% KLD(pnorm(x + interval_size/2, mu, sd) - pnorm(x - interval_size/2, mu, sd), estimated_p)$sum.KLD.px.py)
}
dfc = cbind(dfc, klsc)
#turk
klsc <- vector()
for(i in 1:length(dfc2$binprob)){
estimated_y = strsplit(as.character(dfc2$binprob[i]), ",")
estimated_y = as.numeric(estimated_y[[1]])
guesses = correct_dist %>%
mutate(
estimated_p = y_to_density(c(2,4,6,8,10,12,14,16,18,20), estimated_y)
)
klsc = append(klsc, guesses %$% KLD(pnorm(x + interval_size/2, mu, sd) - pnorm(x - interval_size/2, mu, sd), estimated_p)$sum.KLD.px.py)
}
dfc2 = cbind(dfc2, klsc)
#time
ddply(df, ~condition, summarise, mean=mean(time, na.rm=TRUE), sd=sd(time, na.rm=TRUE))
ddply(df2, ~condition, summarise, mean=mean(time, na.rm=TRUE), sd=sd(time, na.rm=TRUE))
#slider
df2$slider <- as.numeric(paste(df2$slider))
ddply(df, ~condition, summarise, mean=mean(slider, na.rm=TRUE), sd=sd(slider, na.rm=TRUE))
ddply(df2, ~condition, summarise, mean=mean(slider, na.rm=TRUE), sd=sd(slider, na.rm=TRUE))
dfc = rename(dfc, c("klsc"="kl"))
dfc2 = rename(dfc2, c("klsc"="kl"))
adf <- rbind(df20, df50, df100, dfc)
#below line is doing something weird
adf2 <- rbind(df220, df250, df2100, dfc2)
#get rid of erroneous extra emails that create unnecessary levels
adf2 <- droplevels(adf2)
adf$pop = "Univ"
adf2$pop = "Turk"
adfA = rbind(adf, adf2)
adfA$cond_type <- ifelse(adfA$condition=="addclick20" | adfA$condition=="addclick50", "balls_bins", ifelse(adfA$condition=="pullup20" | adfA$condition=="pullup100", "pullup", ifelse(adfA$condition=="roll20" | adfA$condition=="roll100", "roll", ifelse(adfA$condition=="paintdrag20" | adfA$condition=="paintdrag100", "paintdrag", ifelse(adfA$condition=="paintdragtop20" | adfA$condition=="paintdragtop100", "paintdragtop", "continuous")))))
#take the log
adf$logkl <- log(adf$kl)
ddply(adf, ~condition, summarise, mean=mean(logkl, na.rm=TRUE), sd=sd(kl, na.rm=TRUE))
#first dummy code
adfA %<>% mutate(
drawcontfill = ifelse(condition == "drawcontfill", 1, 0),
drawcontline = ifelse(condition == "drawcontline", 1, 0),
addclick20 = ifelse(condition == "addclick20", 1, 0),
addclick50 = ifelse(condition == "addclick50", 1, 0),
pullup20 = ifelse(condition == "pullup20", 1, 0),
pullup100 = ifelse(condition == "pullup100", 1, 0),
roll20 = ifelse(condition == "roll20", 1, 0),
roll100 = ifelse(condition == "roll100", 1, 0),
paintdrag20 = ifelse(condition == "paintdrag20", 1, 0),
paintdrag100 = ifelse(condition == "paintdrag100", 1, 0),
paintdragtop20 = ifelse(condition == "paintdragtop20", 1, 0),
paintdragtop100 = ifelse(condition == "paintdragtop100", 1, 0)
)
library(glmer2stan)
adfA$pop <- as.factor(adfA$pop)
adfA$slider <- as.numeric(adfA$slider)
# construct subject index --- glmer2stan forces you to manage your own cluster indices
adfA$subject_index <- as.integer(as.factor(adfA$email))
adfA$pop_index <- as.integer(as.factor(adfA$pop))
adfA$mturk <- ifelse(adfA$pop=="Univ", 0, 1)
#first in lme4
kl_lme4 <- lmer( log(kl) ~ drawcontfill + drawcontline + addclick20 + pullup20 + pullup100 + roll20 + roll100 + paintdrag20 + paintdrag100 + paintdragtop20 + paintdragtop100 + mturk + (1 | as.factor(adfA$email)) , data=adfA )
#kl_lme4 <- lmer( log(kl) ~ drawcontfill + drawcontline + addclick20 + pullup20 + pullup100 + roll20 + roll100 + paintdrag20 + paintdrag100 + paintdragtop20 + paintdragtop100 + mturk + (1 | adfA$email) , data=adfA, control=lmerControl(check.nobs.vs.nlev="ignore"))
summary(kl_lme4)
# fit with glmer2stan
kl_g2s <- lmer2stan( log(kl) ~ drawcontfill + drawcontline + addclick20 + pullup20 + pullup100 + roll20 + roll100 + paintdrag20 + paintdrag100 + paintdragtop20 + paintdragtop100 + order + mturk + (1 | subject_index) , data=adfA )
stanmer(kl_g2s)
#coef for difference in Univ vs mturk
coef_mturk = extract(kl_g2s) %$% {beta_mturk}
#95 percentile intervals
mturk_posterior_I = PI(coef_mturk, prob=.95)
#posterior distributions for conditions
beta_addclick50_posterior = extract(kl_g2s) %$% {Intercept}
beta_drawcontfill_posterior = extract(kl_g2s) %$% {Intercept + beta_drawcontfill}
beta_drawcontline_posterior = extract(kl_g2s) %$% {Intercept + beta_drawcontline}
beta_addclick20_posterior = extract(kl_g2s) %$% {Intercept + beta_addclick20}
beta_pullup20_posterior = extract(kl_g2s) %$% {Intercept + beta_pullup20}
beta_pullup100_posterior = extract(kl_g2s) %$% {Intercept + beta_pullup100}
beta_roll20_posterior = extract(kl_g2s) %$% {Intercept + beta_roll20}
beta_roll100_posterior = extract(kl_g2s) %$% {Intercept + beta_roll100}
beta_paintdrag20_posterior = extract(kl_g2s) %$% {Intercept + beta_paintdrag20}
beta_paintdrag100_posterior = extract(kl_g2s) %$% {Intercept + beta_paintdrag100}
beta_paintdragtop20_posterior = extract(kl_g2s) %$% {Intercept + beta_paintdragtop20}
beta_paintdragtop100_posterior = extract(kl_g2s) %$% {Intercept + beta_paintdragtop100}
#95 percentile intervals
addclick50_posterior_I = PI(beta_addclick50_posterior, prob=.95)
drawcontfill_posterior_I = PI(beta_drawcontfill_posterior, prob=.95)
drawcontline_posterior_I = PI(beta_drawcontline_posterior, prob=.95)
addclick20_posterior_I = PI(beta_addclick20_posterior, prob=.95)
pullup20_posterior_I = PI(beta_pullup20_posterior, prob=.95)
pullup100_posterior_I = PI(beta_pullup100_posterior, prob=.95)
roll20_posterior_I = PI(beta_roll20_posterior, prob=.95)
roll100_posterior_I = PI(beta_roll100_posterior, prob=.95)
paintdrag20_posterior_I = PI(beta_paintdrag20_posterior, prob=.95)
paintdrag100_posterior_I = PI(beta_paintdrag100_posterior, prob=.95)
paintdragtop20_posterior_I = PI(beta_paintdragtop20_posterior, prob=.95)
paintdragtop100_posterior_I = PI(beta_paintdragtop100_posterior, prob=.95)
#time
#first in lme4
time_lme4 <- lmer( time ~ drawcontfill + drawcontline + addclick20 + pullup20 + pullup100 + roll20 + roll100 + paintdrag20 + paintdrag100 + paintdragtop20 + paintdragtop100 + mturk + (1 | as.factor(adfA$email)) , data=adfA )
summary(time_lme4)
time_g2s <- lmer2stan( time ~ drawcontfill + drawcontline + addclick20 + pullup20 + pullup100 + roll20 + roll100 + paintdrag20 + paintdrag100 + paintdragtop20 + paintdragtop100 + order + mturk + (1 | subject_index) , data=adfA )
stanmer(time_g2s)
#coef for difference in Univ vs mturk
coef_mturk = extract(time_g2s) %$% {beta_mturk}
#95 percentile intervals
mturk_posterior_I = PI(coef_mturk, prob=.95)
#posterior distributions for conditions
beta_addclick50_posterior = extract(time_g2s) %$% {Intercept}
beta_drawcontfill_posterior = extract(time_g2s) %$% {Intercept + beta_drawcontfill}
beta_drawcontline_posterior = extract(time_g2s) %$% {Intercept + beta_drawcontline}
beta_addclick20_posterior = extract(time_g2s) %$% {Intercept + beta_addclick20}
beta_pullup20_posterior = extract(time_g2s) %$% {Intercept + beta_pullup20}
beta_pullup100_posterior = extract(time_g2s) %$% {Intercept + beta_pullup100}
beta_roll20_posterior = extract(time_g2s) %$% {Intercept + beta_roll20}
beta_roll100_posterior = extract(time_g2s) %$% {Intercept + beta_roll100}
beta_paintdrag20_posterior = extract(time_g2s) %$% {Intercept + beta_paintdrag20}
beta_paintdrag100_posterior = extract(time_g2s) %$% {Intercept + beta_paintdrag100}
beta_paintdragtop20_posterior = extract(time_g2s) %$% {Intercept + beta_paintdragtop20}
beta_paintdragtop100_posterior = extract(time_g2s) %$% {Intercept + beta_paintdragtop100}
#95 percentile intervals
addclick50_posterior_I = PI(beta_addclick50_posterior, prob=.95)
drawcontfill_posterior_I = PI(beta_drawcontfill_posterior, prob=.95)
drawcontline_posterior_I = PI(beta_drawcontline_posterior, prob=.95)
addclick20_posterior_I = PI(beta_addclick20_posterior, prob=.95)
pullup20_posterior_I = PI(beta_pullup20_posterior, prob=.95)
pullup100_posterior_I = PI(beta_pullup100_posterior, prob=.95)
roll20_posterior_I = PI(beta_roll20_posterior, prob=.95)
roll100_posterior_I = PI(beta_roll100_posterior, prob=.95)
paintdrag20_posterior_I = PI(beta_paintdrag20_posterior, prob=.95)
paintdrag100_posterior_I = PI(beta_paintdrag100_posterior, prob=.95)
paintdragtop20_posterior_I = PI(beta_paintdragtop20_posterior, prob=.95)
paintdragtop100_posterior_I = PI(beta_paintdragtop100_posterior, prob=.95)
#slider
slider_lme4 <- lmer( slider ~ drawcontfill + drawcontline + addclick20 + pullup20 + pullup100 + roll20 + roll100 + paintdrag20 + paintdrag100 + paintdragtop20 + paintdragtop100 + mturk + (1 | as.factor(adfA$email)) , data=adfA)
summary(slider_lme4)
slider_g2s <- lmer2stan( slider ~ drawcontfill + drawcontline + addclick20 + pullup20 + pullup100 + roll20 + roll100 + paintdrag20 + paintdrag100 + paintdragtop20 + paintdragtop100 + order + mturk + (1 | subject_index) , data=adfA )
stanmer(slider_g2s)
#coef for difference in Univ vs mturk
coef_mturk = extract(slider_g2s) %$% {beta_mturk}
#95 percentile intervals
mturk_posterior_I = PI(coef_mturk, prob=.95)
#posterior distributions for conditions
beta_addclick50_posterior = extract(slider_g2s) %$% {Intercept}
beta_drawcontfill_posterior = extract(slider_g2s) %$% {Intercept + beta_drawcontfill}
beta_drawcontline_posterior = extract(slider_g2s) %$% {Intercept + beta_drawcontline}
beta_addclick20_posterior = extract(slider_g2s) %$% {Intercept + beta_addclick20}
beta_pullup20_posterior = extract(slider_g2s) %$% {Intercept + beta_pullup20}
beta_pullup100_posterior = extract(slider_g2s) %$% {Intercept + beta_pullup100}
beta_roll20_posterior = extract(slider_g2s) %$% {Intercept + beta_roll20}
beta_roll100_posterior = extract(slider_g2s) %$% {Intercept + beta_roll100}
beta_paintdrag20_posterior = extract(slider_g2s) %$% {Intercept + beta_paintdrag20}
beta_paintdrag100_posterior = extract(slider_g2s) %$% {Intercept + beta_paintdrag100}
beta_paintdragtop20_posterior = extract(slider_g2s) %$% {Intercept + beta_paintdragtop20}
beta_paintdragtop100_posterior = extract(slider_g2s) %$% {Intercept + beta_paintdragtop100}
#95 percentile intervals
addclick50_posterior_I = PI(beta_addclick50_posterior, prob=.95)
drawcontfill_posterior_I = PI(beta_drawcontfill_posterior, prob=.95)
drawcontline_posterior_I = PI(beta_drawcontline_posterior, prob=.95)
addclick20_posterior_I = PI(beta_addclick20_posterior, prob=.95)
pullup20_posterior_I = PI(beta_pullup20_posterior, prob=.95)
pullup100_posterior_I = PI(beta_pullup100_posterior, prob=.95)
roll20_posterior_I = PI(beta_roll20_posterior, prob=.95)
roll100_posterior_I = PI(beta_roll100_posterior, prob=.95)
paintdrag20_posterior_I = PI(beta_paintdrag20_posterior, prob=.95)
paintdrag100_posterior_I = PI(beta_paintdrag100_posterior, prob=.95)
paintdragtop20_posterior_I = PI(beta_paintdragtop20_posterior, prob=.95)
paintdragtop100_posterior_I = PI(beta_paintdragtop100_posterior, prob=.95)
packageVersion("ggplot2")
update.packages()
cat("\014")
kl_g2s <- lmer2stan( log(kl) ~ drawcontfill + drawcontline + addclick20 + pullup20 + pullup100 + roll20 + roll100 + paintdrag20 + paintdrag100 + paintdragtop20 + paintdragtop100 + order + mturk + (1 | subject_index) , data=adfA )
stanmer(kl_g2s)
plot(kl_g2s)
version
