})
df2cr$kl_disc_zeros = df2cr$kl_cont_zeros
df2cr %>%
ggplot(aes(x = log(kl_disc), y = log(kl_cont_zeros))) +
geom_point(alpha=.1, pch=19, size=2) +
geom_abline(intercept = 0, slope = 1) +
facet_wrap(~ condition)
adfr <- rbind(df2dr, df2cr)
adsL.first <- adsL[match(unique(adsL$workerId), adsL$workerId),]
worker_prescore <- adsL.first[,c("workerId","prescore")]
adfr = merge(adfr, worker_prescore, by = intersect(names(adfr), names(worker_prescore)))
adfr %>%
mutate(condition = reorder(condition, log(kl_disc))) %>%
ggplot(aes(x = condition, y = log(kl_disc))) +
geom_violin(fill = "gray75") +
geom_boxplot(width = 0.1, color = "red", outlier.color = NA) +
geom_jitter(pch=20, width = .2)
adfr %<>% mutate(
discrete = ifelse(condition == "d_p" | condition == "d_np" | condition == "r_np", 1, 0),
predict = ifelse(condition == "d_p" | condition == "c_p", 1, 0),
rules = ifelse(condition == "rd_np" | condition == "rc_np", 1, 0)
)
ddply(adfr, ~discrete, summarise, median=median(kl_cont_zeros, na.rm=TRUE), sd=sd(kl_cont_zeros, na.rm=TRUE))
ddply(adfr, ~predict, summarise, median=median(kl_cont_zeros, na.rm=TRUE), sd=sd(kl_cont_zeros, na.rm=TRUE))
ddply(adfr, ~rules, summarise, median=median(kl_cont_zeros, na.rm=TRUE), sd=sd(kl_cont_zeros, na.rm=TRUE))
#summarize error in mean and sd of recalled versus shown
#first means
ddply(adfr, ~condition, summarise,
mean=mean(merrs, na.rm=TRUE),
sd=sd(merrs, na.rm=TRUE),
#mean/sd won't be great summaries given the skewed distribution
median = median(merrs, na.rm=TRUE),
mad = mad(merrs, na.rm=TRUE))
#absolute differences means
ddply(adfr, ~condition, summarise,
mean=mean(amerrs, na.rm=TRUE),
sd=sd(amerrs, na.rm=TRUE),
#mean/sd won't be great summaries given the skewed distribution
median = median(amerrs, na.rm=TRUE),
mad = mad(amerrs, na.rm=TRUE))
#sds
ddply(adfr, ~condition, summarise,
mean=mean(serrs, na.rm=TRUE),
sd=sd(serrs, na.rm=TRUE),
#mean/sd won't be great summaries given the skewed distribution
median = median(serrs, na.rm=TRUE),
mad = mad(serrs, na.rm=TRUE))
#absolute difference sds
ddply(adfr, ~condition, summarise,
mean=mean(aserrs, na.rm=TRUE),
sd=sd(aserrs, na.rm=TRUE),
#mean/sd won't be great summaries given the skewed distribution
median = median(aserrs, na.rm=TRUE),
mad = mad(aserrs, na.rm=TRUE))
#plot for overview of how many perfectly recalled in each condition (dots at 0 on y)
ggplot(adfr, aes(x=reorder(condition, amerrs, FUN=median), amerrs)) + geom_boxplot() + geom_jitter()
#distribution of errors in means - recalled
adfr %>%
ggplot(aes(x = means - 250)) +
stat_density() +
facet_wrap(~condition) +
geom_vline(xintercept = 0, color="black") +
theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(),
panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
labs(x="Recalled Mean - Reference Mean",  y="") +
geom_vline(xintercept = 18, color="red")
#use red for sample sampling dist, black for pop sampling dist, blue for predictive, orange for data dist
#distribution of errors in sd - recalled
adfr %>%
ggplot(aes(x = sds - 150/sqrt(40))) + stat_density() +
facet_wrap(~condition) +
geom_vline(xintercept = 0, color="black") + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + labs(x="Recalled SD - Reference SD",  y="") + geom_vline(xintercept = 2.85, color="red") #+ geom_vline(xintercept = 144, color="orange")
#counts of how many people perfectly recalled in discrete
by(adfr, adfr$condition, function(means) sum(means==250, na.rm=TRUE))
#by percentage
by(adfr, adfr$condition, function(means) sum(means==250, na.rm=TRUE)/nrow(means))
#first, set seed so results don't differ slightly each time from convergence process
#center prescore
adfr$prescore_c <- adfr$prescore - mean(adfr$prescore)
adfr_gamlss = adfr %>%
#gamlss needs all columns not to have NAs
dplyr::select(discrete, predict, rules, prescore_c, kl_disc, kl_disc_zeros, kl_cont_zeros)
mg = gamlss(log(kl_cont_zeros) ~ discrete*predict + rules + prescore_c,
sigma.formula = ~ discrete*predict + rules + prescore_c,
data=adfr_gamlss)
summary(mg)
kld_model_spec = alist(
kl ~ dlnorm(mu, sigma),
mu <- mu_a + mu_bd*discrete + mu_bp*predict + mu_bdp*discrete*predict + mu_br*rules + mu_bpr*prescore_c,
log(sigma) <- sigma_a + sigma_bd*discrete + sigma_bp*predict + sigma_bdp*discrete*predict +
sigma_br*rules + sigma_bpr*prescore_c,
mu_a ~ dnorm(0,5), #mean, sd
mu_bd ~ dnorm(0,5),
mu_bp ~ dnorm(0,5),
mu_bdp ~ dnorm(0,5),
mu_br ~ dnorm(0,5),
mu_bpr ~ dnorm(0,5),
sigma_a ~ dnorm(0,2.5),
sigma_bd ~ dnorm(0,2.5),
sigma_bp ~ dnorm(0,2.5),
sigma_bdp ~ dnorm(0,2.5),
sigma_br ~ dnorm(0,2.5),
sigma_bpr ~ dnorm(0,2.5)
)
m_recall_disc = map2stan(kld_model_spec, data = adfr_gamlss %>% rename(kl = kl_disc))
m_recall_disc = map2stan(kld_model_spec, data = adfr_gamlss)
m_recall_disc_zeros = map2stan(kld_model_spec, data = adfr_gamlss %>% rename(kl = kl_disc_zeros))
head(adfr_gamlss)
coef_display_order = rev(c("intercept", "discrete", "predict", "discrete*predict", "rules", "literacy"))
plot_kld_model_mu_coefs(m_recall_disc) + xlim(-2, 2) + ggtitle("Method 1")
plot_kld_model_mu_coefs = function(m) {
mu_coefs = m %>%
extract.samples() %>%
as_data_frame() %>%
transmute(
intercept = mu_a,
discrete = mu_bd,
predict = mu_bp,
`discrete*predict` = mu_bdp,
rules = mu_br,
literacy = mu_bpr
) %>%
mutate(iteration = 1:n()) %>%
gather(condition, estimate, -iteration) %>%
mutate(condition = fct_relevel(condition, coef_display_order))
mu_coefs %>%
ggplot(aes(x = estimate, y = condition)) +
geom_violinh(fill = "black", color = NA) +
geom_vline(xintercept = 0, color = "red") +
theme(axis.text=element_text(size=18),
# axis.text.y=element_blank(),
panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
labs(x = "Coefficients (mu)", y = "")
}
plot_kld_model_mu_coefs(m_recall_disc) + xlim(-2, 2) + ggtitle("Method 1")
head(adfr_gamlss)
cat("\014")
library(dplyr)
import::from(plyr, ddply, rename)
library(magrittr)   #pipe syntax (%>%, %<>%, etc)
library(ggplot2)
library(lme4)
library(forcats)
library(tidyr)
import::from(gamlss.dist, dTF, qTF, pTF, rTF)   #the TF functions are a scaled and shifted t distribution
import::from(LaplacesDemon, KLD)
import::from(MASS, parcoord)
# for RStan installation instructions, see https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started
# DO NOT attempt to install rstan using install.packages(), as it will likely fail. Unfortunately its
# installation is finicky, and we recommend following the instructions on that page to the letter.
library(rstan)
library(brms)
theme_set(theme_bw())
options(mc.cores = parallel::detectCores())
mu = 10
sd = 2
min_x = 0
max_x = 20
interval_size = 2
#derive discrete reference distribution
make_discrete_reference_dist = function(dist_fun, min_x, max_x, interval_size) {
data_frame(
x_min = seq(min_x, max_x - interval_size, by = interval_size),
x_max = seq(min_x + interval_size, max_x, by = interval_size),
x = (x_min + x_max)/2,   #midpoint of the interval
#this difference sometimes comes out as 0 due to rounding errors in bins with very low probability
#adjust it to always be at least epsilon (smallest value > 0 on this machine
#such that x + eps != x is always true).
p = pmax(.Machine$double.eps, dist_fun(x_max) - dist_fun(x_min))
)
}
#derive discrete analog
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
#remember to change to interval_size=1 for 100 balls
#check what it looks like:
correct_dist %>%
ggplot(aes(x = x, y = p)) +
stat_function(fun = function(x) dnorm(x, mu, sd) * interval_size) +
geom_segment(aes(xend = x, y = 0, yend = p), size = 2)
df <- read.csv("anonymized_survey_response_dists_with_order_forR_infovis.txt", sep="\t", row.names=NULL)
df2 <- read.csv("anonymized_survey_turk_response_dists_with_order_forR_infovis.txt", sep="\t", row.names=NULL)
colnames(df)
df20 <- subset(df, df$nballs=="20")
df50 = subset(df, df$nballs=="50")
df100 = subset(df, df$nballs=="100")
df220 <- subset(df2, df2$nballs=="20")
df250 = subset(df2, df2$nballs=="50")
df2100 = subset(df2, df2$nballs=="100")
#for 20 and 50 circles
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
#function to calculate
kld_discrete = function(reference_dist, guesses) {
#responses with 20 balls appear to be recorded as being out of 100, so
#instead of assuming some number here we'll just make this out of the
#total of the supplied guesses
guesses = guesses / sum(guesses)
smoothing_denominator = 1000
n_atoms = nrow(correct_dist)
#make sure the number of supplied guesses is the same as the atoms in the reference dist
stopifnot(n_atoms == length(guesses))
#calculate smoothed guesses (guaranteed to have no 0s and sum to 1)
estimated_p = (guesses * smoothing_denominator + 1) / (smoothing_denominator + n_atoms)
#assert we have normalized everything correctly (within a reasonable tolerance)
# stopifnot(isTRUE(all.equal(sum(reference_dist$p), 1, tolerance = 0.00001)))
#  stopifnot(isTRUE(all.equal(sum(estimated_p), 1, tolerance = 0.00001)))
#since we have normalized everything correctly, we can use the KLD formula directly
#instead of calling KLD (which renormalizes again)
sum(reference_dist$p * (log(reference_dist$p) - log(estimated_p)))
}
#Note that we must convert binprob, which loosk like 0,0,5,15,30,30,15,5,0,0, into a vector for each data set
#HCI listserv participants (df)
df20$kl = apply(df20, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df50$kl = apply(df50, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
#MTurk participants (df2)
df220$kl = apply(df220, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df250$kl = apply(df250, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
#Change interval size and recreate reference distribution
interval_size = 1
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
df100$kl = apply(df100, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df2100$kl = apply(df2100, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
min_x = 0
max_x = 20
mu = 10
sd = 2
interval_size = 2
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
y_to_density = function(x, estimated_y) {
user_density = approxfun(x, estimated_y)
area_under_curve = integrate(user_density, lower = min(x), upper = max(x))$value
#finally, a similar smoothing as applied in the discrete case to ensure there are no 0s
#see kl-divergence-for-discrete-dist.Rmd
#might want to revisit this smoothing in the other files and make them all exactly the same?
#(i.e. here we assume fractions out of 1000, while in the other file we used 100 and 20 depending
#on number of balls --- but this could make the KLD higher for those conditions just because of
#differences in smoothing parameters)
(estimated_y / area_under_curve * 1000 + 1) / (1000 + length(x))
}
#Divide data in HCI listserv and MTurk participants
dfc=subset(df, df$condition == "drawcontline" | df$condition=="drawcontfill")
dfc2=subset(df2, df2$condition == "drawcontline" | df2$condition=="drawcontfill")
klsc <- vector()
for(i in 1:length(dfc$binprob)){
estimated_y = strsplit(as.character(dfc$binprob[i]), ",")
estimated_y = as.numeric(estimated_y[[1]])
#interval_size = 1
guesses = correct_dist %>%
mutate(
estimated_p = y_to_density(c(2,4,6,8,10,12,14,16,18,20), estimated_y)
)
klsc = append(klsc, guesses %$% KLD(pnorm(x + interval_size/2, mu, sd) - pnorm(x - interval_size/2, mu, sd), estimated_p)$sum.KLD.px.py)
}
dfc = cbind(dfc, klsc)
#MTurk participants
klsc <- vector()
for(i in 1:length(dfc2$binprob)){
estimated_y = strsplit(as.character(dfc2$binprob[i]), ",")
estimated_y = as.numeric(estimated_y[[1]])
guesses = correct_dist %>%
mutate(
estimated_p = y_to_density(c(2,4,6,8,10,12,14,16,18,20), estimated_y)
)
klsc = append(klsc, guesses %$% KLD(pnorm(x + interval_size/2, mu, sd) - pnorm(x - interval_size/2, mu, sd), estimated_p)$sum.KLD.px.py)
}
dfc2 = cbind(dfc2, klsc)
#time by sample
ddply(df, ~condition, summarise, mean=mean(time, na.rm=TRUE), sd=sd(time, na.rm=TRUE))
ddply(df2, ~condition, summarise, mean=mean(time, na.rm=TRUE), sd=sd(time, na.rm=TRUE))
#slider by sample
#first transform to numeric
df2$slider <- as.numeric(paste(df2$slider))
ddply(df, ~condition, summarise, mean=mean(slider, na.rm=TRUE), sd=sd(slider, na.rm=TRUE))
ddply(df2, ~condition, summarise, mean=mean(slider, na.rm=TRUE), sd=sd(slider, na.rm=TRUE))
dfc = rename(dfc, c("klsc"="kl"))
dfc2 = rename(dfc2, c("klsc"="kl"))
adf <- rbind(df20, df50, df100, dfc)
#below line is doing something weird
adf2 <- rbind(df220, df250, df2100, dfc2)
#get rid of erroneous extra emails that create unnecessary levels
adf2 <- droplevels(adf2)
adf$pop = "Univ"
adf2$pop = "Turk"
adfA = rbind(adf, adf2)
#adfA$cond_type <- ifelse(adfA$condition=="addclick20" | adfA$condition=="addclick50", "balls_bins", ifelse(adfA$condition=="pullup20" | adfA$condition=="pullup100", "pullup", ifelse(adfA$condition=="roll20" | adfA$condition=="roll100", "roll", ifelse(adfA$condition=="paintdrag20" | adfA$condition=="paintdrag100", "paintdrag", ifelse(adfA$condition=="paintdragtop20" | adfA$condition=="paintdragtop100", "paintdragtop", "continuous")))))
#take the log
adf$logkl <- log(adf$kl)
ddply(adf, ~condition, summarise, mean=mean(logkl, na.rm=TRUE), sd=sd(kl, na.rm=TRUE))
adfA %<>% mutate(
# move addlcick50 to the front to make it the intercept in the dummy-coded model
condition = fct_relevel(condition, "addclick50"),
pop = factor(pop),
slider = as.numeric(slider),
subject = factor(email),
mturk = pop != "Univ"
)
kl_lme4 <- lmer( log(kl) ~ condition + mturk + (1 | subject), data = adfA)
summary(kl_lme4)
kl_g2s <- brm(log(kl) ~ condition + order + mturk + (1 | subject), data = adfA)
kl_g2s
stanplot(kl_g2s)
adfA %>%
expand(condition, order = 1, mturk = FALSE) %>%
cbind(fitted(kl_g2s, newdata = ., re_formula = NA))
adfA %>%
expand(condition, order = 1, mturk = FALSE) %>%
cbind(fitted(kl_g2s, newdata = ., re_formula = NA))
help("tidyr")
help(expand)
cat("\014")
library(dplyr)
import::from(plyr, ddply, rename)
library(magrittr)   #pipe syntax (%>%, %<>%, etc)
library(ggplot2)
library(lme4)
library(forcats)
library(tidyr)
import::from(gamlss.dist, dTF, qTF, pTF, rTF)   #the TF functions are a scaled and shifted t distribution
import::from(LaplacesDemon, KLD)
import::from(MASS, parcoord)
# for RStan installation instructions, see https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started
# DO NOT attempt to install rstan using install.packages(), as it will likely fail. Unfortunately its
# installation is finicky, and we recommend following the instructions on that page to the letter.
library(rstan)
library(brms)
theme_set(theme_bw())
options(mc.cores = parallel::detectCores())
mu = 10
sd = 2
min_x = 0
max_x = 20
interval_size = 2
#derive discrete reference distribution
make_discrete_reference_dist = function(dist_fun, min_x, max_x, interval_size) {
data_frame(
x_min = seq(min_x, max_x - interval_size, by = interval_size),
x_max = seq(min_x + interval_size, max_x, by = interval_size),
x = (x_min + x_max)/2,   #midpoint of the interval
#this difference sometimes comes out as 0 due to rounding errors in bins with very low probability
#adjust it to always be at least epsilon (smallest value > 0 on this machine
#such that x + eps != x is always true).
p = pmax(.Machine$double.eps, dist_fun(x_max) - dist_fun(x_min))
)
}
#derive discrete analog
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
#remember to change to interval_size=1 for 100 balls
#check what it looks like:
correct_dist %>%
ggplot(aes(x = x, y = p)) +
stat_function(fun = function(x) dnorm(x, mu, sd) * interval_size) +
geom_segment(aes(xend = x, y = 0, yend = p), size = 2)
df <- read.csv("anonymized_survey_response_dists_with_order_forR_infovis.txt", sep="\t", row.names=NULL)
df2 <- read.csv("anonymized_survey_turk_response_dists_with_order_forR_infovis.txt", sep="\t", row.names=NULL)
colnames(df)
df20 <- subset(df, df$nballs=="20")
df50 = subset(df, df$nballs=="50")
df100 = subset(df, df$nballs=="100")
df220 <- subset(df2, df2$nballs=="20")
df250 = subset(df2, df2$nballs=="50")
df2100 = subset(df2, df2$nballs=="100")
#for 20 and 50 circles
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
#function to calculate
kld_discrete = function(reference_dist, guesses) {
#responses with 20 balls appear to be recorded as being out of 100, so
#instead of assuming some number here we'll just make this out of the
#total of the supplied guesses
guesses = guesses / sum(guesses)
smoothing_denominator = 1000
n_atoms = nrow(correct_dist)
#make sure the number of supplied guesses is the same as the atoms in the reference dist
stopifnot(n_atoms == length(guesses))
#calculate smoothed guesses (guaranteed to have no 0s and sum to 1)
estimated_p = (guesses * smoothing_denominator + 1) / (smoothing_denominator + n_atoms)
#assert we have normalized everything correctly (within a reasonable tolerance)
# stopifnot(isTRUE(all.equal(sum(reference_dist$p), 1, tolerance = 0.00001)))
#  stopifnot(isTRUE(all.equal(sum(estimated_p), 1, tolerance = 0.00001)))
#since we have normalized everything correctly, we can use the KLD formula directly
#instead of calling KLD (which renormalizes again)
sum(reference_dist$p * (log(reference_dist$p) - log(estimated_p)))
}
#Note that we must convert binprob, which loosk like 0,0,5,15,30,30,15,5,0,0, into a vector for each data set
#HCI listserv participants (df)
df20$kl = apply(df20, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df50$kl = apply(df50, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
#MTurk participants (df2)
df220$kl = apply(df220, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df250$kl = apply(df250, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
#Change interval size and recreate reference distribution
interval_size = 1
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
df100$kl = apply(df100, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
df2100$kl = apply(df2100, 1, function(row) {
guesses = as.numeric(strsplit(as.character(row["binprob"]), ",")[[1]])
kld_discrete(correct_dist, guesses)
})
min_x = 0
max_x = 20
mu = 10
sd = 2
interval_size = 2
correct_dist = make_discrete_reference_dist(function(x) pnorm(x, mu, sd), min_x, max_x, interval_size)
y_to_density = function(x, estimated_y) {
user_density = approxfun(x, estimated_y)
area_under_curve = integrate(user_density, lower = min(x), upper = max(x))$value
#finally, a similar smoothing as applied in the discrete case to ensure there are no 0s
#see kl-divergence-for-discrete-dist.Rmd
#might want to revisit this smoothing in the other files and make them all exactly the same?
#(i.e. here we assume fractions out of 1000, while in the other file we used 100 and 20 depending
#on number of balls --- but this could make the KLD higher for those conditions just because of
#differences in smoothing parameters)
(estimated_y / area_under_curve * 1000 + 1) / (1000 + length(x))
}
#Divide data in HCI listserv and MTurk participants
dfc=subset(df, df$condition == "drawcontline" | df$condition=="drawcontfill")
dfc2=subset(df2, df2$condition == "drawcontline" | df2$condition=="drawcontfill")
klsc <- vector()
for(i in 1:length(dfc$binprob)){
estimated_y = strsplit(as.character(dfc$binprob[i]), ",")
estimated_y = as.numeric(estimated_y[[1]])
#interval_size = 1
guesses = correct_dist %>%
mutate(
estimated_p = y_to_density(c(2,4,6,8,10,12,14,16,18,20), estimated_y)
)
klsc = append(klsc, guesses %$% KLD(pnorm(x + interval_size/2, mu, sd) - pnorm(x - interval_size/2, mu, sd), estimated_p)$sum.KLD.px.py)
}
dfc = cbind(dfc, klsc)
#MTurk participants
klsc <- vector()
for(i in 1:length(dfc2$binprob)){
estimated_y = strsplit(as.character(dfc2$binprob[i]), ",")
estimated_y = as.numeric(estimated_y[[1]])
guesses = correct_dist %>%
mutate(
estimated_p = y_to_density(c(2,4,6,8,10,12,14,16,18,20), estimated_y)
)
klsc = append(klsc, guesses %$% KLD(pnorm(x + interval_size/2, mu, sd) - pnorm(x - interval_size/2, mu, sd), estimated_p)$sum.KLD.px.py)
}
dfc2 = cbind(dfc2, klsc)
#time by sample
ddply(df, ~condition, summarise, mean=mean(time, na.rm=TRUE), sd=sd(time, na.rm=TRUE))
ddply(df2, ~condition, summarise, mean=mean(time, na.rm=TRUE), sd=sd(time, na.rm=TRUE))
#slider by sample
#first transform to numeric
df2$slider <- as.numeric(paste(df2$slider))
ddply(df, ~condition, summarise, mean=mean(slider, na.rm=TRUE), sd=sd(slider, na.rm=TRUE))
ddply(df2, ~condition, summarise, mean=mean(slider, na.rm=TRUE), sd=sd(slider, na.rm=TRUE))
dfc = rename(dfc, c("klsc"="kl"))
dfc2 = rename(dfc2, c("klsc"="kl"))
adf <- rbind(df20, df50, df100, dfc)
#below line is doing something weird
adf2 <- rbind(df220, df250, df2100, dfc2)
#get rid of erroneous extra emails that create unnecessary levels
adf2 <- droplevels(adf2)
adf$pop = "Univ"
adf2$pop = "Turk"
adfA = rbind(adf, adf2)
#adfA$cond_type <- ifelse(adfA$condition=="addclick20" | adfA$condition=="addclick50", "balls_bins", ifelse(adfA$condition=="pullup20" | adfA$condition=="pullup100", "pullup", ifelse(adfA$condition=="roll20" | adfA$condition=="roll100", "roll", ifelse(adfA$condition=="paintdrag20" | adfA$condition=="paintdrag100", "paintdrag", ifelse(adfA$condition=="paintdragtop20" | adfA$condition=="paintdragtop100", "paintdragtop", "continuous")))))
#take the log
adf$logkl <- log(adf$kl)
ddply(adf, ~condition, summarise, mean=mean(logkl, na.rm=TRUE), sd=sd(kl, na.rm=TRUE))
adfA %<>% mutate(
# move addlcick50 to the front to make it the intercept in the dummy-coded model
condition = fct_relevel(condition, "addclick50"),
pop = factor(pop),
slider = as.numeric(slider),
subject = factor(email),
mturk = pop != "Univ"
)
kl_lme4 <- lmer( log(kl) ~ condition + mturk + (1 | subject), data = adfA)
summary(kl_lme4)
kl_g2s <- brm(log(kl) ~ condition + order + mturk + (1 | subject), data = adfA)
kl_g2s
adfA %>%
expand(condition, order = 1, mturk = FALSE) %>%
cbind(fitted(kl_g2s, newdata = ., re_formula = NA))
install.packages("tidyr")
install.packages("tidyr")
